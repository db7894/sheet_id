{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.transform import rotate\n",
    "from random import shuffle, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CELLPHONE_IMG_PATH = os.path.join(os.getcwd(), 'cellphone_imgs')\n",
    "DB_PATH = os.path.join(os.getcwd(), 'png_imgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \n",
    "crop = \n",
    "crop_size = \n",
    "MAX_ITERATION = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class seg_dataset_reader:\n",
    "    path = \"\"\n",
    "    class_mappings = \"\"\n",
    "    files = []\n",
    "    images = []\n",
    "    annotations = []\n",
    "    test_images = []\n",
    "    test_annotations = []\n",
    "    batch_offset = 0\n",
    "    epochs_completed = 0\n",
    "\n",
    "    def __init__(self, data_path, max_pages=40, crop=True, crop_size=[1000,1000], test_size=20):\n",
    "        \"\"\"\n",
    "        Initialize a file reader for the classification data\n",
    "        :param records_list: path to the dataset\n",
    "        sample record: {'image': f, 'annotation': annotation_file, 'filename': filename}\n",
    "        \"\"\"\n",
    "        print(\"Initializing Dataset Reader...\")\n",
    "        self.path = data_path\n",
    "        self.max_pages = max_pages\n",
    "        self.crop = crop\n",
    "        self.crop_size = crop_size\n",
    "        self.test_size = test_size\n",
    "\n",
    "        images_list = []\n",
    "        images_glob = os.path.join(self.path, \"images_png\", '*.' + 'png')\n",
    "        images_list.extend(glob.glob(images_glob))\n",
    "\n",
    "        #shuffle image list\n",
    "        shuffle(images_list)\n",
    "\n",
    "        if max_pages is None:\n",
    "            max_pages = len(images_list)\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        if max_pages > len(images_list):\n",
    "            print(\"Not enough data, only \" + str(len(images_list)) + \" available\")\n",
    "\n",
    "        if test_size >= max_pages:\n",
    "            print(\"Test set too big (\"+str(test_size)+\"), max_pages is: \"+str(max_pages))\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        print(\"Splitting dataset, train: \"+str(max_pages-test_size)+\" images, test: \"+str(test_size)+ \" images\")\n",
    "        test_image_list = images_list[0:test_size]\n",
    "        train_image_list = images_list[test_size:max_pages]\n",
    "        \n",
    "#         test_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in test_image_list]\n",
    "#         train_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in train_image_list]\n",
    "            \n",
    "        self._read_images(test_image_list,train_image_list)\n",
    "\n",
    "    def _read_images(self,test_image_list,train_image_list, annotations=True):\n",
    "\n",
    "        dat_train = [self._transform(filename) for filename in train_image_list]\n",
    "        for dat in dat_train:\n",
    "            self.images.append(dat[0])\n",
    "            self.annotations.append(dat[1])\n",
    "        self.images = np.array(self.images)\n",
    "        self.images = np.expand_dims(self.images, -1)\n",
    "\n",
    "        self.annotations = np.array(self.annotations)\n",
    "        self.annotations = np.expand_dims(self.annotations, -1)\n",
    "\n",
    "        print(\"Training set done\")\n",
    "        dat_test = [self._transform(filename) for filename in test_image_list]\n",
    "        for dat in dat_test:\n",
    "            self.test_images.append(dat[0])\n",
    "            self.test_annotations.append(dat[1])\n",
    "        self.test_images = np.array(self.test_images)\n",
    "        self.test_images = np.expand_dims(self.test_images, -1)\n",
    "\n",
    "        self.test_annotations = np.array(self.test_annotations)\n",
    "        self.test_annotations = np.expand_dims(self.test_annotations, -1)\n",
    "        print(\"Test set done\")\n",
    "\n",
    "\n",
    "    def _transform(self, filename):\n",
    "        image = misc.imread(filename)\n",
    "        annotation = misc.imread(filename.replace(\"/images_png/\", \"/pix_annotations_png/\")) #these are images/annotations\n",
    "        print(\"im working!\" + str(randint(0,10)))\n",
    "        if not image.shape[0:2] == annotation.shape[0:2]:\n",
    "            print(\"input and annotation have different sizes!\")\n",
    "            import sys\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            sys.exit(1)\n",
    "\n",
    "        if image.shape[-1] != 1:\n",
    "            # take mean over color channels, image BW anyways --> fix in dataset creation\n",
    "            image = np.mean(image, -1)\n",
    "\n",
    "        if self.crop:\n",
    "            coord_0 = randint(0, (image.shape[0] - self.crop_size[0]))\n",
    "            coord_1 = randint(0, (image.shape[1] - self.crop_size[1]))\n",
    "\n",
    "            image = image[coord_0:(coord_0+self.crop_size[0]),coord_1:(coord_1+self.crop_size[1])]\n",
    "            annotation = annotation[coord_0:(coord_0 + self.crop_size[0]), coord_1:(coord_1 + self.crop_size[1])]\n",
    "\n",
    "        return [image, annotation]\n",
    "    \n",
    "\n",
    "    def get_records(self):\n",
    "        return self.images, self.annotations\n",
    "\n",
    "    def reset_batch_offset(self, offset=0):\n",
    "        self.batch_offset = offset\n",
    "\n",
    "    def get_test_records(self):\n",
    "        return self.test_images, self.test_annotations\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.batch_offset\n",
    "        self.batch_offset += batch_size\n",
    "        if self.batch_offset > self.images.shape[0]:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            print(\"****************** Epochs completed: \" + str(self.epochs_completed) + \"******************\")\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.images.shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            self.images = self.images[perm]\n",
    "            self.annotations = self.annotations[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.batch_offset = batch_size\n",
    "\n",
    "        end = self.batch_offset\n",
    "        return self.images[start:end], self.annotations[start:end]\n",
    "\n",
    "    def get_random_batch(self, batch_size):\n",
    "        indexes = np.random.randint(0, self.images.shape[0], size=[batch_size]).tolist()\n",
    "        return self.images[indexes], self.annotations[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    data_reader = seg_dataset_reader(data_dir, crop=crop, crop_size=crop_size)\n",
    "    \n",
    "    #Placeholders for FeedDict\n",
    "    keep_probability_conv = tf.placeholder(tf.float32, name=\"keep_probability_conv\")\n",
    "    image = tf.placeholder(tf.float32, shape=[None, FLAGS.crop_size[0], FLAGS.crop_size[0], 1], name=\"image\")\n",
    "    annotation = tf.placeholder(tf.int32, shape=[None, FLAGS.crop_size[0], FLAGS.crop_size[0], 1], name=\"labels\")\n",
    "\n",
    "    # Apply FCN or model    \n",
    "    \n",
    "    for itr in range(step, MAX_ITERATION):\n",
    "        train_images, train_annotations= data_reader.next_batch(FLAGS.batch_size)\n",
    "        feed_dict = {image: train_images, annotation: train_annotations, keep_probability_conv: 0.85}\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "        print(itr)\n",
    "\n",
    "        if itr % 10 == 0:\n",
    "            \"\"\" get train loss \"\"\"\n",
    "            #train_loss = sess.run([loss], feed_dict=feed_dict)\n",
    "            print(\"Step: %d, Train_loss: %g\" % (itr, train_loss[0]))\n",
    "\n",
    "        if itr % 500 == 0 and itr != 0:\n",
    "            \"\"\" get valid loss here \"\"\"\n",
    "            valid_images, valid_annotations = data_reader.get_test_records()\n",
    "            #valid_loss = sess.run(loss, feed_dict={image: valid_images, annotation: valid_annotations, keep_probability_conv: 1.0})\n",
    "            print(\"%s ---> Validation_loss: %g\" % (datetime.datetime.now(), valid_loss))\n",
    "            saver.save(sess, FLAGS.logs_dir + \"model.ckpt\", itr)\n",
    "    a,b = data_reader.get_test_records()\n",
    "    #valid_loss, output = sess.run([loss, pred_annotation], feed_dict={image: a, annotation: b, keep_probability_conv: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "png_paths = glob.glob(os.path.join(DB_PATH, '*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = {}\n",
    "for i in range(len(png_paths)):\n",
    "    img = cv2.imread(png_paths[i], 0)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    db[png_paths[i]] = (kp, des)\n",
    "    print(\n",
    "        \"Finish computing SIFT descriptor {:}/{:}\".format(\n",
    "            i + 1, len(png_paths)),\n",
    "        file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_paths = sorted(glob.glob(os.path.join(CELLPHONE_IMG_PATH, '*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(query_paths[0], 0)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), 'groundtruth.csv'))\n",
    "groundtruth = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (cellphone_img, sheet_img) in df.values:\n",
    "    groundtruth[cellphone_img] = sheet_img + '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(db_png_paths,\n",
    "           des_query,\n",
    "           expected_match,\n",
    "           threshold_npairs=25,\n",
    "           verbose=False):\n",
    "    scoreList = []\n",
    "    for idx in range(len(db_png_paths)):\n",
    "        ref_path = db_png_paths[idx]\n",
    "        #matches = flann.knnMatch(db[ref_path][1], des_query, k=2)\n",
    "\n",
    "        totalDistance = 0\n",
    "        counterGood = 0\n",
    "        # ratio test as per Lowe's paper\n",
    "        for i, (m, n) in enumerate(matches):\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                counterGood += 1\n",
    "                totalDistance += m.distance\n",
    "\n",
    "        scoreList.append({\n",
    "            'path': ref_path,\n",
    "            'distance': totalDistance,\n",
    "            'n_pairs': counterGood,\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Finish searching {:}/{:} distance = {:} (# good pairs = {:})\".\n",
    "                format(idx + 1, len(png_paths), totalDistance, counterGood),\n",
    "                file=sys.stderr)\n",
    "    filteredScore = [\n",
    "        score for score in scoreList if score['n_pairs'] > threshold_npairs\n",
    "    ]\n",
    "\n",
    "    sortedScore = sorted(filteredScore, key=lambda x: x['distance'])\n",
    "    for score in scoreList:\n",
    "        if score['n_pairs'] <= threshold_npairs:\n",
    "            sortedScore.append(score)\n",
    "\n",
    "    rank = 1\n",
    "    for score in sortedScore:\n",
    "        if (os.path.split(score['path'])[1] == expected_match):\n",
    "            return rank\n",
    "        rank += 1\n",
    "\n",
    "    print(\"Expected match = {:}\".format(expected_match), file=sys.stderr)\n",
    "    print(sortedScore, file=sys.stderr)\n",
    "    raise ValueError(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MRR = 0\n",
    "top1acc = 0\n",
    "query_num = 0\n",
    "for query_path in query_paths:\n",
    "    img_query = cv2.imread(query_path, 0)\n",
    "    #kp_query, des_query = sift.detectAndCompute(img, None)\n",
    "    expected_match = groundtruth[os.path.split(query_path)[1]]\n",
    "    rank = search(png_paths, des_query, expected_match)\n",
    "\n",
    "    MRR += (1 / len(query_paths)) * (1 / rank)\n",
    "    top1acc += (1 / len(query_paths)) * (rank == 1)\n",
    "\n",
    "    query_num += 1\n",
    "    print(\"Query {:} : rank = {:}\".format(query_num, rank), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print((\"MRR = {:}\".format(MRR)))\n",
    "print((\"Top-1 accuracy = {:}\".format(top1acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Experiment               | MRR           | top-1 accuracy  |\n",
    "| --------------           | ------------- | --------------- |\n",
    "| SIFT (0 threshold)       | 0.01          | 0               |\n",
    "| SIFT (50 threshold)      | 0.05          | 0.025           |\n",
    "| SIFT (100 threshold)     | 0.03          | 0               |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
