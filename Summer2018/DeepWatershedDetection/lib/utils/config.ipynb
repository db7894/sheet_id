{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Fast R-CNN\n",
    "# Copyright (c) 2015 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ross Girshick\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\"\"\"Fast R-CNN config system.\n",
    "This file specifies default config options for Fast R-CNN. You should not\n",
    "change values in this file. Instead, you should write a config file (in yaml)\n",
    "and use cfg_from_file(yaml_file) to load it and override the default options.\n",
    "Most tools in $ROOT/tools take a --cfg option to specify an override file.\n",
    "    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n",
    "    - See experiments/cfgs/*.yml for example YAML config override files\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "# `pip install easydict` if you don't have it\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "__C = edict()\n",
    "# Consumers can get config by:\n",
    "#   from fast_rcnn_config import cfg\n",
    "cfg = __C\n",
    "\n",
    "#\n",
    "# Training options\n",
    "#\n",
    "\n",
    "__C.TRAIN = edict()\n",
    "\n",
    "# Scales to use during training (can list multiple scales)\n",
    "# Each scale is the pixel size of an image's shortest side\n",
    "__C.TRAIN.SCALES = (600,)\n",
    "\n",
    "# Max pixel size of the longest side of a scaled input image\n",
    "__C.TRAIN.MAX_SIZE = 1000\n",
    "\n",
    "# Images to use per minibatch\n",
    "__C.TRAIN.IMS_PER_BATCH = 2\n",
    "\n",
    "# Minibatch size (number of regions of interest [ROIs])\n",
    "__C.TRAIN.BATCH_SIZE = 128\n",
    "\n",
    "# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n",
    "__C.TRAIN.FG_FRACTION = 0.25\n",
    "\n",
    "# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n",
    "__C.TRAIN.FG_THRESH = 0.5\n",
    "\n",
    "# Overlap threshold for a ROI to be considered background (class = 0 if\n",
    "# overlap in [LO, HI))\n",
    "__C.TRAIN.BG_THRESH_HI = 0.5\n",
    "__C.TRAIN.BG_THRESH_LO = 0.1\n",
    "\n",
    "# Use horizontally-flipped images during training?\n",
    "__C.TRAIN.USE_FLIPPED = True\n",
    "\n",
    "# Train bounding-box regressors\n",
    "__C.TRAIN.BBOX_REG = True\n",
    "\n",
    "# Overlap required between a ROI and ground-truth box in order for that ROI to\n",
    "# be used as a bounding-box regression training example\n",
    "__C.TRAIN.BBOX_THRESH = 0.5\n",
    "\n",
    "# Iterations between snapshots\n",
    "__C.TRAIN.SNAPSHOT_ITERS = 10000\n",
    "\n",
    "# solver.prototxt specifies the snapshot path prefix, this adds an optional\n",
    "# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n",
    "__C.TRAIN.SNAPSHOT_INFIX = ''\n",
    "\n",
    "# Use a prefetch thread in roi_data_layer.layer\n",
    "# So far I haven't found this useful; likely more engineering work is required\n",
    "__C.TRAIN.USE_PREFETCH = False\n",
    "\n",
    "# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n",
    "__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n",
    "# Deprecated (inside weights)\n",
    "__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "# Normalize the targets using \"precomputed\" (or made up) means and stdevs\n",
    "# (BBOX_NORMALIZE_TARGETS must also be True)\n",
    "__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = False\n",
    "__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n",
    "__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n",
    "\n",
    "# Train using these proposals\n",
    "__C.TRAIN.PROPOSAL_METHOD = 'selective_search'\n",
    "\n",
    "# Make minibatches from images that have similar aspect ratios (i.e. both\n",
    "# tall and thin or both short and wide) in order to avoid wasting computation\n",
    "# on zero-padding.\n",
    "__C.TRAIN.ASPECT_GROUPING = True\n",
    "\n",
    "# Use RPN to detect objects\n",
    "__C.TRAIN.HAS_RPN = False\n",
    "# IOU >= thresh: positive example\n",
    "__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n",
    "# IOU < thresh: negative example\n",
    "__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n",
    "# If an anchor statisfied by positive and negative conditions set to negative\n",
    "__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n",
    "# Max number of foreground examples\n",
    "__C.TRAIN.RPN_FG_FRACTION = 0.5\n",
    "# Total number of examples\n",
    "__C.TRAIN.RPN_BATCHSIZE = 256\n",
    "# NMS threshold used on RPN proposals\n",
    "__C.TRAIN.RPN_NMS_THRESH = 0.7\n",
    "# Number of top scoring boxes to keep before apply NMS to RPN proposals\n",
    "__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n",
    "# Number of top scoring boxes to keep after applying NMS to RPN proposals\n",
    "__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n",
    "# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n",
    "__C.TRAIN.RPN_MIN_SIZE = 16\n",
    "# Deprecated (outside weights)\n",
    "__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "# Give the positive RPN examples weight of p * 1 / {num positives}\n",
    "# and give negatives a weight of (1 - p)\n",
    "# Set to -1.0 to use uniform example weighting\n",
    "__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n",
    "\n",
    "\n",
    "#\n",
    "# Testing options\n",
    "#\n",
    "\n",
    "__C.TEST = edict()\n",
    "\n",
    "# Scales to use during testing (can list multiple scales)\n",
    "# Each scale is the pixel size of an image's shortest side\n",
    "__C.TEST.SCALES = (600,)\n",
    "\n",
    "# Max pixel size of the longest side of a scaled input image\n",
    "__C.TEST.MAX_SIZE = 1000\n",
    "\n",
    "# Overlap threshold used for non-maximum suppression (suppress boxes with\n",
    "# IoU >= this threshold)\n",
    "__C.TEST.NMS = 0.3\n",
    "\n",
    "# Experimental: treat the (K+1) units in the cls_score layer as linear\n",
    "# predictors (trained, eg, with one-vs-rest SVMs).\n",
    "__C.TEST.SVM = False\n",
    "\n",
    "# Test using bounding-box regressors\n",
    "__C.TEST.BBOX_REG = True\n",
    "\n",
    "# Propose boxes\n",
    "__C.TEST.HAS_RPN = False\n",
    "\n",
    "# Test using these proposals\n",
    "__C.TEST.PROPOSAL_METHOD = 'selective_search'\n",
    "\n",
    "## NMS threshold used on RPN proposals\n",
    "__C.TEST.RPN_NMS_THRESH = 0.7\n",
    "## Number of top scoring boxes to keep before apply NMS to RPN proposals\n",
    "__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n",
    "## Number of top scoring boxes to keep after applying NMS to RPN proposals\n",
    "__C.TEST.RPN_POST_NMS_TOP_N = 300\n",
    "# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n",
    "__C.TEST.RPN_MIN_SIZE = 16\n",
    "\n",
    "\n",
    "#\n",
    "# MISC\n",
    "#\n",
    "\n",
    "# The mapping from image coordinates to feature map coordinates might cause\n",
    "# some boxes that are distinct in image space to become identical in feature\n",
    "# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n",
    "# for identifying duplicate boxes.\n",
    "# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n",
    "__C.DEDUP_BOXES = 1./16.\n",
    "\n",
    "# Pixel mean values (BGR order) as a (1, 1, 3) array\n",
    "# We use the same pixel mean for all networks even though it's not exactly what\n",
    "# they were trained with\n",
    "__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "\n",
    "# For reproducibility\n",
    "__C.RNG_SEED = 3\n",
    "\n",
    "# A small number that's used many times\n",
    "__C.EPS = 1e-14\n",
    "\n",
    "# Root directory of project\n",
    "__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), '..', '..'))\n",
    "\n",
    "# Data directory\n",
    "__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, 'data'))\n",
    "\n",
    "# Model directory\n",
    "__C.MODELS_DIR = osp.abspath(osp.join(__C.ROOT_DIR, 'models', 'pascal_voc'))\n",
    "\n",
    "# Name (or path to) the matlab executable\n",
    "__C.MATLAB = 'matlab'\n",
    "\n",
    "# Place outputs under an experiments directory\n",
    "__C.EXP_DIR = 'default'\n",
    "\n",
    "# Use GPU implementation of non-maximum suppression\n",
    "__C.USE_GPU_NMS = True\n",
    "\n",
    "# Default GPU device id\n",
    "__C.GPU_ID = 0\n",
    "\n",
    "\n",
    "def get_output_dir(imdb, net=None):\n",
    "    \"\"\"Return the directory where experimental artifacts are placed.\n",
    "    If the directory does not exist, it is created.\n",
    "    A canonical path is built using the name from an imdb and a network\n",
    "    (if not None).\n",
    "    \"\"\"\n",
    "    outdir = osp.abspath(osp.join(__C.ROOT_DIR, 'output', __C.EXP_DIR, imdb.name))\n",
    "    if net is not None:\n",
    "        outdir = osp.join(outdir, net.name)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    return outdir\n",
    "\n",
    "def _merge_a_into_b(a, b):\n",
    "    \"\"\"Merge config dictionary a into config dictionary b, clobbering the\n",
    "    options in b whenever they are also specified in a.\n",
    "    \"\"\"\n",
    "    if type(a) is not edict:\n",
    "        return\n",
    "\n",
    "    for k, v in a.iteritems():\n",
    "        # a must specify keys that are in b\n",
    "        if not b.has_key(k):\n",
    "            raise KeyError('{} is not a valid config key'.format(k))\n",
    "\n",
    "        # the types must match, too\n",
    "        old_type = type(b[k])\n",
    "        if old_type is not type(v):\n",
    "            if isinstance(b[k], np.ndarray):\n",
    "                v = np.array(v, dtype=b[k].dtype)\n",
    "            else:\n",
    "                raise ValueError(('Type mismatch ({} vs. {}) '\n",
    "                                'for config key: {}').format(type(b[k]),\n",
    "                                                            type(v), k))\n",
    "\n",
    "        # recursively merge dicts\n",
    "        if type(v) is edict:\n",
    "            try:\n",
    "                _merge_a_into_b(a[k], b[k])\n",
    "            except:\n",
    "                print('Error under config key: {}'.format(k))\n",
    "                raise\n",
    "        else:\n",
    "            b[k] = v\n",
    "\n",
    "def cfg_from_file(filename):\n",
    "    \"\"\"Load a config file and merge it into the default options.\"\"\"\n",
    "    import yaml\n",
    "    with open(filename, 'r') as f:\n",
    "        yaml_cfg = edict(yaml.load(f))\n",
    "\n",
    "    _merge_a_into_b(yaml_cfg, __C)\n",
    "\n",
    "def cfg_from_list(cfg_list):\n",
    "    \"\"\"Set config keys via list (e.g., from command line).\"\"\"\n",
    "    from ast import literal_eval\n",
    "    assert len(cfg_list) % 2 == 0\n",
    "    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n",
    "        key_list = k.split('.')\n",
    "        d = __C\n",
    "        for subkey in key_list[:-1]:\n",
    "            assert d.has_key(subkey)\n",
    "            d = d[subkey]\n",
    "        subkey = key_list[-1]\n",
    "        assert d.has_key(subkey)\n",
    "        try:\n",
    "            value = literal_eval(v)\n",
    "        except:\n",
    "            # handle the case when v is a string literal\n",
    "            value = v\n",
    "        assert type(value) == type(d[subkey]), \\\n",
    "            'type {} does not match original type {}'.format(\n",
    "            type(value), type(d[subkey]))\n",
    "        d[subkey] = value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
